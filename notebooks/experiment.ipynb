{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Advanced RAG Comparison - Experiment Notebook\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Building all three retriever types\n",
    "2. Testing with sample queries\n",
    "3. Running comprehensive evaluation\n",
    "4. Generating benchmark results for Streamlit\n",
    "\n",
    "**Methods Compared:**\n",
    "- Basic RAG (baseline)\n",
    "- Sentence Window Retrieval\n",
    "- Auto-Merging Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.config import Config\n",
    "from src.utils import (\n",
    "    load_documents,\n",
    "    setup_rag_system,\n",
    "    print_query_results,\n",
    "    create_sample_eval_questions,\n",
    ")\n",
    "from src.retrievers import (\n",
    "    build_basic_retriever,\n",
    "    build_sentence_window_retriever,\n",
    "    build_auto_merging_retriever,\n",
    ")\n",
    "from src.evaluation import (\n",
    "    RetrieverEvaluator,\n",
    "    compare_retrievers,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print current configuration\n",
    "Config.print_config()\n",
    "\n",
    "# Validate configuration\n",
    "Config.validate_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Initialize System Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup RAG system (LLM, embeddings, reranker)\n",
    "system = setup_rag_system()\n",
    "\n",
    "llm = system[\"llm\"]\n",
    "embed_model = system[\"embed_model\"]\n",
    "reranker = system[\"reranker\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from data directory\n",
    "documents = load_documents()\n",
    "\n",
    "print(f\"\\nüìä Document Statistics:\")\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"Total characters: {sum(len(doc.text) for doc in documents):,}\")\n",
    "print(f\"Average doc length: {sum(len(doc.text) for doc in documents) / len(documents):,.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî® Build Retrievers\n",
    "\n",
    "Build all three retriever types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic RAG (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_retriever = build_basic_retriever(\n",
    "    documents=documents,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    reranker=reranker,\n",
    "    index_name=\"basic_index\",\n",
    "    force_rebuild=False,  # Set to True to rebuild\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ {basic_retriever.get_retriever_name()} ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentence Window Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_retriever = build_sentence_window_retriever(\n",
    "    documents=documents,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    reranker=reranker,\n",
    "    window_size=3,  # Try 1, 3, 5, 7\n",
    "    index_name=\"sentence_window_index\",\n",
    "    force_rebuild=False,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ {sentence_window_retriever.get_retriever_name()} ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Auto-Merging Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_merging_retriever = build_auto_merging_retriever(\n",
    "    documents=documents,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    reranker=reranker,\n",
    "    chunk_sizes=[2048, 512, 128],  # Try different hierarchies\n",
    "    index_name=\"auto_merging_index\",\n",
    "    force_rebuild=False,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ {auto_merging_retriever.get_retriever_name()} ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test Single Query\n",
    "\n",
    "Test a single query across all retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"What is the main topic of the document?\"\n",
    "\n",
    "retrievers = {\n",
    "    \"Basic RAG\": basic_retriever,\n",
    "    \"Sentence Window\": sentence_window_retriever,\n",
    "    \"Auto-Merging\": auto_merging_retriever,\n",
    "}\n",
    "\n",
    "for name, retriever in retrievers.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    response, nodes = retriever.query(test_query, return_nodes=True)\n",
    "    \n",
    "    print(f\"\\nüí¨ Response:\\n{response}\\n\")\n",
    "    print(f\"üìö Retrieved {len(nodes)} nodes\")\n",
    "    \n",
    "    # Show first retrieved context\n",
    "    if nodes:\n",
    "        print(f\"\\nüìÑ Top Context (snippet):\")\n",
    "        print(nodes[0].node.get_content()[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Comprehensive Evaluation\n",
    "\n",
    "Evaluate all retrievers on multiple questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Test Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your test questions\n",
    "test_questions = [\n",
    "    \"What is the main topic discussed in the document?\",\n",
    "    \"What are the key findings or conclusions?\",\n",
    "    \"What methodology was used?\",\n",
    "    \"What are the main recommendations?\",\n",
    "    \"What are the limitations mentioned?\",\n",
    "]\n",
    "\n",
    "# Or load from file\n",
    "# import json\n",
    "# with open('../data/eval_questions.json', 'r') as f:\n",
    "#     questions_data = json.load(f)\n",
    "#     test_questions = [q['question'] for q in questions_data]\n",
    "\n",
    "print(f\"üìù Test questions: {len(test_questions)}\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"   {i}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluator\n",
    "evaluator = RetrieverEvaluator()\n",
    "\n",
    "# Evaluate all retrievers\n",
    "results = evaluator.evaluate_multiple_retrievers(\n",
    "    retrievers=[basic_retriever, sentence_window_retriever, auto_merging_retriever],\n",
    "    questions=test_questions,\n",
    "    ground_truths=None,  # Add if you have ground truth answers\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comparison dataframe\n",
    "comparison_df = evaluator.get_comparison_dataframe()\n",
    "\n",
    "print(\"\\nüìä Comparison Results:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Highlight best scores\n",
    "styled_df = comparison_df.style.highlight_max(\n",
    "    subset=['faithfulness', 'answer_relevancy', 'context_relevancy'],\n",
    "    color='lightgreen'\n",
    ").highlight_min(\n",
    "    subset=['avg_response_time'],\n",
    "    color='lightgreen'\n",
    ")\n",
    "\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results for Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to be used in Streamlit dashboard\n",
    "evaluator.save_results(Config.STORAGE_DIR / \"eval_results.json\")\n",
    "\n",
    "print(\"‚úÖ Results saved! You can now view them in the Streamlit dashboard.\")\n",
    "print(\"\\nRun: streamlit run streamlit_app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Experiment: Different Window Sizes\n",
    "\n",
    "Compare different sentence window sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes = [1, 3, 5, 7]\n",
    "window_retrievers = []\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    print(f\"\\nü™ü Building retriever with window_size={window_size}...\")\n",
    "    \n",
    "    retriever = build_sentence_window_retriever(\n",
    "        documents=documents,\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        reranker=reranker,\n",
    "        window_size=window_size,\n",
    "        index_name=f\"sentence_window_{window_size}\",\n",
    "        force_rebuild=False,\n",
    "    )\n",
    "    \n",
    "    window_retrievers.append(retriever)\n",
    "\n",
    "print(\"\\n‚úÖ All window size retrievers built!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate window size variants\n",
    "window_evaluator = RetrieverEvaluator()\n",
    "\n",
    "window_results = window_evaluator.evaluate_multiple_retrievers(\n",
    "    retrievers=window_retrievers,\n",
    "    questions=test_questions[:3],  # Use subset for faster testing\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Compare results\n",
    "window_comparison = window_evaluator.get_comparison_dataframe()\n",
    "display(window_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Experiment: Different Chunk Sizes (Auto-Merging)\n",
    "\n",
    "Compare different hierarchical chunk configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_configs = [\n",
    "    [2048, 512, 128],\n",
    "    [1024, 256, 64],\n",
    "    [4096, 1024, 256],\n",
    "]\n",
    "\n",
    "chunk_retrievers = []\n",
    "\n",
    "for chunk_sizes in chunk_configs:\n",
    "    print(f\"\\nüîÑ Building retriever with chunk_sizes={chunk_sizes}...\")\n",
    "    \n",
    "    retriever = build_auto_merging_retriever(\n",
    "        documents=documents,\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        reranker=reranker,\n",
    "        chunk_sizes=chunk_sizes,\n",
    "        index_name=f\"auto_merge_{'_'.join(map(str, chunk_sizes))}\",\n",
    "        force_rebuild=False,\n",
    "    )\n",
    "    \n",
    "    chunk_retrievers.append(retriever)\n",
    "\n",
    "print(\"\\n‚úÖ All chunk size retrievers built!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate chunk size variants\n",
    "chunk_evaluator = RetrieverEvaluator()\n",
    "\n",
    "chunk_results = chunk_evaluator.evaluate_multiple_retrievers(\n",
    "    retrievers=chunk_retrievers,\n",
    "    questions=test_questions[:3],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Compare results\n",
    "chunk_comparison = chunk_evaluator.get_comparison_dataframe()\n",
    "display(chunk_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot metrics comparison\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'context_relevancy']\n",
    "x = range(len(comparison_df))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    if metric in comparison_df.columns:\n",
    "        plt.bar(\n",
    "            [xi + width * i for xi in x],\n",
    "            comparison_df[metric],\n",
    "            width=width,\n",
    "            label=metric.replace('_', ' ').title()\n",
    "        )\n",
    "\n",
    "plt.xlabel('Retriever')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Retriever Performance Comparison')\n",
    "plt.xticks([xi + width for xi in x], comparison_df['retriever'])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Next Steps\n",
    "\n",
    "1. **Add More Documents**: Place PDF/TXT files in the `data/` directory\n",
    "2. **Customize Questions**: Create your own evaluation questions\n",
    "3. **Tune Parameters**: Experiment with different:\n",
    "   - Window sizes (1, 3, 5, 7, 9)\n",
    "   - Chunk sizes ([2048, 512, 128], [1024, 256, 64], etc.)\n",
    "   - Top-K values\n",
    "   - Reranker settings\n",
    "4. **View Dashboard**: Run `streamlit run streamlit_app.py` to see interactive results\n",
    "5. **Compare Methods**: Analyze which method works best for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ‚úÖ Building three advanced RAG retrieval methods\n",
    "- ‚úÖ Evaluating with RAGAS metrics (faithfulness, answer relevancy, context relevancy)\n",
    "- ‚úÖ Comparing performance across methods\n",
    "- ‚úÖ Experimenting with different configurations\n",
    "- ‚úÖ Generating results for Streamlit dashboard\n",
    "\n",
    "**Key Findings:**\n",
    "- Basic RAG provides a good baseline\n",
    "- Sentence Window adds context richness\n",
    "- Auto-Merging balances granularity and context\n",
    "\n",
    "Choose the method that best fits your:\n",
    "- Document type\n",
    "- Query complexity\n",
    "- Latency requirements\n",
    "- Quality needs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
